@article{bradskyOpenCVLibrary2000,
  title = {The {{OpenCV Library}}},
  author = {Bradsky, G},
  year = {2000},
  journal = {Dr. Dobb's Journal of Software Tools},
  number = {120},
  pages = {122--125}
}

@misc{cherianFreeMoCapFreeOpen2024,
  title = {{{FreeMoCap}}: {{A}} Free, Open Source Markerless Motion Capture System},
  shorttitle = {{{FreeMoCap}}},
  author = {Cherian, Aaron and Queen, Philip and Trent, Wirth and Endurance, Idehen and Matthis, Jonathan Samir},
  year = {2024},
  month = jan,
  doi = {10.5281/zenodo.7233714},
  urldate = {2024-01-13},
  abstract = {Free Motion Capture for Everyone ðŸ’€âœ¨},
  copyright = {AGPL-3.0}
}

@misc{cinematographydatabaseIndieViconSystem2021,
  title = {Indie {{Vicon System Cost}} and {{Solo Operator Guide}}},
  author = {{Cinematography Database}},
  year = {2021},
  month = nov,
  urldate = {2024-01-11},
  abstract = {The number one question I get about Vicon is how much does the system cost?  And it depends, but I cover the cost of my Indie Vicon system and then show the basics of operating the system and using the Shogun software. 00:00 Intro 00:15 Indie Vicon System Cost 04:13 Optical MOCAP 101 06:08 Calibration Wand 09:24 MOCAP Suit 13:28 Standard Deviation HMC 15:28 Hands 19:46 A Pose and Range of Motion 21:29 Summary}
}

@article{karashchukAniposeToolkitRobust2021,
  title = {Anipose: {{A}} Toolkit for Robust Markerless {{3D}} Pose Estimation},
  shorttitle = {Anipose},
  author = {Karashchuk, Pierre and Rupp, Katie L. and Dickinson, Evyn S. and {Walling-Bell}, Sarah and Sanders, Elischa and Azim, Eiman and Brunton, Bingni W. and Tuthill, John C.},
  year = {2021},
  month = sep,
  journal = {Cell Reports},
  volume = {36},
  number = {13},
  publisher = {{Elsevier}},
  issn = {2211-1247},
  doi = {10.1016/j.celrep.2021.109730},
  urldate = {2024-01-12},
  langid = {english},
  pmid = {34592148},
  keywords = {3D,behavior,camera calibration,deep learning,Drosophila joint rotation,markerless tracking,neuroscience,pose estimation,robust tracking,visualization},
  file = {C:\Users\Mac Prible\Zotero\storage\UDCC7SXE\Karashchuk et al. - 2021 - Anipose A toolkit for robust markerless 3D pose e.pdf}
}

@misc{lugaresiMediaPipeFrameworkBuilding2019,
  title = {{{MediaPipe}}: {{A Framework}} for {{Building Perception Pipelines}}},
  shorttitle = {{{MediaPipe}}},
  author = {Lugaresi, Camillo and Tang, Jiuqiang and Nash, Hadon and McClanahan, Chris and Uboweja, Esha and Hays, Michael and Zhang, Fan and Chang, Chuo-Ling and Yong, Ming Guang and Lee, Juhyun and Chang, Wan-Teh and Hua, Wei and Georg, Manfred and Grundmann, Matthias},
  year = {2019},
  month = jun,
  number = {arXiv:1906.08172},
  eprint = {1906.08172},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1906.08172},
  urldate = {2024-01-12},
  abstract = {Building applications that perceive the world around them is challenging. A developer needs to (a) select and develop corresponding machine learning algorithms and models, (b) build a series of prototypes and demos, (c) balance resource consumption against the quality of the solutions, and finally (d) identify and mitigate problematic cases. The MediaPipe framework addresses all of these challenges. A developer can use MediaPipe to build prototypes by combining existing perception components, to advance them to polished cross-platform applications and measure system performance and resource consumption on target platforms. We show that these features enable a developer to focus on the algorithm or model development and use MediaPipe as an environment for iteratively improving their application with results reproducible across different devices and platforms. MediaPipe will be open-sourced at https://github.com/google/mediapipe.},
  archiveprefix = {arxiv},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing}},
  file = {C\:\\Users\\Mac Prible\\Zotero\\storage\\N686INWQ\\Lugaresi et al. - 2019 - MediaPipe A Framework for Building Perception Pip.pdf;C\:\\Users\\Mac Prible\\Zotero\\storage\\AQ4JVMNL\\1906.html}
}

@article{mathisDeepLabCutMarkerlessPose2018,
  title = {{{DeepLabCut}}: Markerless Pose Estimation of User-Defined Body Parts with Deep Learning},
  shorttitle = {{{DeepLabCut}}},
  author = {Mathis, Alexander and Mamidanna, Pranav and Cury, Kevin M. and Abe, Taiga and Murthy, Venkatesh N. and Mathis, Mackenzie Weygandt and Bethge, Matthias},
  year = {2018},
  month = sep,
  journal = {Nature Neuroscience},
  volume = {21},
  number = {9},
  pages = {1281--1289},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-018-0209-y},
  urldate = {2024-01-12},
  abstract = {Quantifying behavior is crucial for many applications in neuroscience. Videography provides easy methods for the observation and recording of animal behavior in diverse settings, yet extracting particular aspects of a behavior for further analysis can be highly time consuming. In motor control studies, humans or other animals are often marked with reflective markers to assist with computer-based tracking, but markers are intrusive, and the number and location of the markers must be determined a priori. Here we present an efficient method for markerless pose estimation based on transfer learning with deep neural networks that achieves excellent results with minimal training data. We demonstrate the versatility of this framework by tracking various body parts in multiple species across a broad collection of behaviors. Remarkably, even when only a small number of frames are labeled ({\textasciitilde}200), the algorithm achieves excellent tracking performance on test frames that is comparable to human accuracy.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {Behavioural methods,Computational neuroscience,Machine learning}
}

@misc{mayorovLargescaleBundleAdjustment,
  title = {Large-Scale Bundle Adjustment in Scipy {\textemdash} {{SciPy Cookbook}} Documentation},
  author = {Mayorov, Nikolay},
  journal = {SciPy Cookbook},
  urldate = {2024-01-11},
  howpublished = {https://scipy-cookbook.readthedocs.io/items/bundle\_adjustment.html},
  file = {C:\Users\Mac Prible\Zotero\storage\AQRHQ6S3\bundle_adjustment.html}
}

@misc{MediapipeHolistic,
  title = {Mediapipe {{Holistic}}},
  journal = {GitHub},
  urldate = {2024-01-13},
  abstract = {Cross-platform, customizable ML solutions for live and streaming media. - google/mediapipe},
  howpublished = {https://github.com/google/mediapipe/blob/master/docs/solutions/holistic.md},
  langid = {english},
  file = {C:\Users\Mac Prible\Zotero\storage\BZ28DCT6\holistic.html}
}

@misc{mmposecontributorsOpenMMLabPoseEstimation2020,
  title = {{{OpenMMLab Pose Estimation Toolbox}} and {{Benchmark}}},
  author = {{MMPose Contributors}},
  year = {2020},
  month = aug,
  urldate = {2024-01-13},
  abstract = {OpenMMLab Pose Estimation Toolbox and Benchmark.},
  copyright = {Apache-2.0}
}

@article{virtanenSciPyFundamentalAlgorithms2020,
  title = {{{SciPy}} 1.0: Fundamental Algorithms for Scientific Computing in {{Python}}},
  shorttitle = {{{SciPy}} 1.0},
  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and Van Der Walt, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C J and Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and Van Mulbregt, Paul and {SciPy 1.0 Contributors} and Vijaykumar, Aditya and Bardelli, Alessandro Pietro and Rothberg, Alex and Hilboll, Andreas and Kloeckner, Andreas and Scopatz, Anthony and Lee, Antony and Rokem, Ariel and Woods, C. Nathan and Fulton, Chad and Masson, Charles and H{\"a}ggstr{\"o}m, Christian and Fitzgerald, Clark and Nicholson, David A. and Hagen, David R. and Pasechnik, Dmitrii V. and Olivetti, Emanuele and Martin, Eric and Wieser, Eric and Silva, Fabrice and Lenders, Felix and Wilhelm, Florian and Young, G. and Price, Gavin A. and Ingold, Gert-Ludwig and Allen, Gregory E. and Lee, Gregory R. and Audren, Herv{\'e} and Probst, Irvin and Dietrich, J{\"o}rg P. and Silterra, Jacob and Webber, James T and Slavi{\v c}, Janko and Nothman, Joel and Buchner, Johannes and Kulick, Johannes and Sch{\"o}nberger, Johannes L. and De Miranda Cardoso, Jos{\'e} Vin{\'i}cius and Reimer, Joscha and Harrington, Joseph and Rodr{\'i}guez, Juan Luis Cano and {Nunez-Iglesias}, Juan and Kuczynski, Justin and Tritz, Kevin and Thoma, Martin and Newville, Matthew and K{\"u}mmerer, Matthias and Bolingbroke, Maximilian and Tartre, Michael and Pak, Mikhail and Smith, Nathaniel J. and Nowaczyk, Nikolai and Shebanov, Nikolay and Pavlyk, Oleksandr and Brodtkorb, Per A. and Lee, Perry and McGibbon, Robert T. and Feldbauer, Roman and Lewis, Sam and Tygier, Sam and Sievert, Scott and Vigna, Sebastiano and Peterson, Stefan and More, Surhud and Pudlik, Tadeusz and Oshima, Takuya and Pingel, Thomas J. and Robitaille, Thomas P. and Spura, Thomas and Jones, Thouis R. and Cera, Tim and Leslie, Tim and Zito, Tiziano and Krauss, Tom and Upadhyay, Utkarsh and Halchenko, Yaroslav O. and {V{\'a}zquez-Baeza}, Yoshiki},
  year = {2020},
  month = mar,
  journal = {Nature Methods},
  volume = {17},
  number = {3},
  pages = {261--272},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-019-0686-2},
  urldate = {2024-01-12},
  abstract = {Abstract             SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments.},
  langid = {english},
  file = {C:\Users\Mac Prible\Zotero\storage\KW66959G\Virtanen et al. - 2020 - SciPy 1.0 fundamental algorithms for scientific c.pdf}
}
